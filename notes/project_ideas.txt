TO STUDY GESTURES WRT COMMUNICATION, COGNITION

- automatically develop taxonomy of gestures?
  - what could be done with such gestures?

- analyse gestures at moments that are considered cognitively heavy
  - are there gesture properties that can help identify such moments?
  - what does it mean to be cognitively heavy?
  - focus on a certain kind of cognition (ie spatial reasoning, logical reasoning, grasping of abstract concept..?)
  - can we predict through gesture whether a person has grasped a certain concept?

- surrounding moments of fluid collaboration
  - what is collaboration?
  - fluid turn taking?
  - positively mentioning fellow participants?
  - elaborating the ideas of fellow participants?

- surrounding moments of poor collaboration
- at moments that convey certain kinds of ideas
- how they relate to specific dialog acts?
- how they relate to emotions?
- which elements most critical to resolve ambiguous turn transitions?
- can gesture be used to predict psychological / mental / physical health, troubles?
- can gesture be used to predict relationship satisfaction between friends, couples?

TO STUDY SOCIAL SIGNAL PROCESSING
- how do certain social signals within goal oriented group relate to obtention of desired outcomes
  - empathy?
  - presence of a leader?
  - rapport?

could contact Michel Sasseville for potential collaboration with CRP????
  - video of session, participant survey afterwards
  - comparison of group dynamics here vs group dynamics in business meetings or traditional lecture contexts


STUDY EYE GAZE AND READING

- can eye gaze be used to predict misunderstood text passages?
  - could be many types of misunderstandings (misinterpretation, confusion, inattention, etc)
- how then could this be used to give feedback to user?
  - automatically generate questions regarding misunderstood passages?
    - this could happen on a section-by-section level in an instructional text, for example.
    - give user instructions to read. after each section ask them to repeat instructions. Thus get labels for what they correctly understood and what they didnt
    - system first tries to predict what was understood and what wasn't; possibly tries to categorize different types of misunderstandings
  - automatically extract question-answer pairs relating to the misunderstood passages. user gets personalized quiz!
- not only WHAT they didn't understand, but WHY
- lots of work has been done regarding question-answer pairs.
- in second language context, use gaze to predict and automatically translate misunderstood words
- compare expert gaze to beginner gaze
- does user find text coherent? interesting?


MICKEY:
EMPATHY
Philippe's dataset with rapport rating
what features does he have? gaze, facial expression, hand motion, prosodic features
leadership, competence,
game dataset?


GESTURE AND CONCEPTS
actor videos with emotion annotations
mickey will ask philippe for both datasets!!!


Meeting with Harsha:
text saliency

personality and ego
acting
great speakers


Multimodal affect recognition with potential focus on gesture features
promising data:
Phillip, GEMEP for affect recognition
analyze contribution of different features to affect recognition, relations between emotions

ELEA, HGI, HUMAINE, AMI for social signal processing
affect and disagreement?
affect and leadership?
affect and dominance?
affect and rapport?
affect and empathy?

social

authors to look into:

cigdem beyan
daniel gatica-perez


verify automatic pose labels using IAA.
echo fad





echo fat
echo fat
echo fat
echo fat
echo fat
echo fat
echo fat
